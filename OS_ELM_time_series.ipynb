{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MISJasonChuang/EDD_ELM/blob/main/OS_ELM_time_series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I09KmeI3l81M"
      },
      "outputs": [],
      "source": [
        "#  --quiet hide code cell output\n",
        "!pip install statsmodels --quiet\n",
        "!pip install river"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Module to build Extreme Learning Machine (ELM) models\"\"\"\n",
        "\n",
        "# ===================================================\n",
        "# Acknowledgement:\n",
        "# Author: David C. Lambert [dcl -at- panix -dot- com]\n",
        "# Copyright(c) 2013\n",
        "# License: Simple BSD\n",
        "# ===================================================\n",
        "\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.utils import as_float_array\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    \"ELMRegressor\",\n",
        "    \"ELMClassifier\",\n",
        "    \"GenELMRegressor\",\n",
        "    \"GenELMClassifier\",\n",
        "]\n",
        "\n",
        "\n",
        "class BaseELM(BaseEstimator):\n",
        "    \"\"\"Abstract Base class for ELMs\"\"\"\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self, hidden_layer, regressor):\n",
        "        self.regressor = regressor\n",
        "        self.hidden_layer = hidden_layer\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class GenELMRegressor(BaseELM, RegressorMixin):\n",
        "    \"\"\"\n",
        "    Regression model based on Extreme Learning Machine.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `hidden_layer` : random_layer instance, optional\n",
        "        (default=MLPRandomLayer(random_state=0))\n",
        "\n",
        "    `regressor`    : regressor instance, optional\n",
        "        (default=sklearn.linear_model.LinearRegression())\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `coefs_` : numpy array\n",
        "        Fitted regression coefficients if no regressor supplied.\n",
        "\n",
        "    `fitted_` : bool\n",
        "        Flag set when fit has been called already.\n",
        "\n",
        "    `hidden_activations_` : numpy array of shape [n_samples, n_hidden]\n",
        "        Hidden layer activations for last input.\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    ELMRegressor, MLPRandomLayer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_layer=None, regressor=None):\n",
        "        if hidden_layer is None:\n",
        "            # Default value\n",
        "            hidden_layer = MLPRandomLayer(random_state=0)\n",
        "        elif not isinstance(hidden_layer, RandomLayer):\n",
        "            raise ValueError(\"Argument 'hidden_layer' must be a RandomLayer instance\")\n",
        "\n",
        "        if regressor is None:\n",
        "            # Default value\n",
        "            regressor = LinearRegression()\n",
        "        elif not isinstance(regressor, RegressorMixin):\n",
        "            raise ValueError(\"Argument 'regressor' must be a RegressorMixin instance\")\n",
        "\n",
        "        super(GenELMRegressor, self).__init__(hidden_layer, regressor)\n",
        "\n",
        "        self.coefs_ = None\n",
        "        self.fitted_ = False\n",
        "        self.hidden_activations_ = None\n",
        "\n",
        "    def _fit_regression(self, y):\n",
        "        \"\"\"Fit regression with the supplied regressor\"\"\"\n",
        "        self.regressor.fit(self.hidden_activations_, y)\n",
        "        self.fitted_ = True\n",
        "\n",
        "    @property\n",
        "    def is_fitted(self):\n",
        "        \"\"\"Check if model was fitted\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            boolean, True if model is fitted\n",
        "        \"\"\"\n",
        "        return self.fitted_\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "        # fit random hidden layer and compute the hidden layer activations\n",
        "        self.hidden_activations_ = self.hidden_layer.fit_transform(X)\n",
        "\n",
        "        # solve the regression from hidden activations to outputs\n",
        "        self._fit_regression(as_float_array(y, copy=True))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"GenELMRegressor not fitted\")\n",
        "\n",
        "        # compute hidden layer activations\n",
        "        self.hidden_activations_ = self.hidden_layer.transform(X)\n",
        "\n",
        "        # compute output predictions for new hidden activations\n",
        "        predictions = self.regressor.predict(self.hidden_activations_)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "class GenELMClassifier(BaseELM, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Classification model based on Extreme Learning Machine.\n",
        "    Internally, it uses a GenELMRegressor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `hidden_layer` : random_layer instance, optional\n",
        "        (default=MLPRandomLayer(random_state=0))\n",
        "\n",
        "    `binarizer`    : LabelBinarizer, optional\n",
        "        (default=sklearn.preprocessing.LabelBinarizer(-1, 1))\n",
        "\n",
        "    `regressor`    : regressor instance, optional\n",
        "        (default=LinearRegression())\n",
        "        Used to perform the regression from hidden unit activations\n",
        "        to the outputs and subsequent predictions.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `classes_` : numpy array of shape [n_classes]\n",
        "        Array of class labels\n",
        "\n",
        "    `genelm_regressor_` : ELMRegressor instance\n",
        "        Performs actual fit of binarized values\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    GenELMRegressor, ELMClassifier, MLPRandomLayer\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_layer=None, binarizer=None, regressor=None):\n",
        "\n",
        "        # Default values\n",
        "        if hidden_layer is None:\n",
        "            hidden_layer = MLPRandomLayer(random_state=0)\n",
        "        if binarizer is None:\n",
        "            binarizer = LabelBinarizer(neg_label=-1, pos_label=1)\n",
        "\n",
        "        super(GenELMClassifier, self).__init__(hidden_layer, regressor)\n",
        "\n",
        "        self.binarizer = binarizer\n",
        "\n",
        "        self.classes_ = None\n",
        "        self._genelm_regressor = GenELMRegressor(hidden_layer, regressor)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "        self.classes_ = np.unique(y)\n",
        "\n",
        "        y_bin = self.binarizer.fit_transform(y)\n",
        "\n",
        "        self._genelm_regressor.fit(X, y_bin)\n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def is_fitted(self):\n",
        "        \"\"\"Check if model was fitted\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            boolean, True if model is fitted\n",
        "        \"\"\"\n",
        "        return self._genelm_regressor is not None and self._genelm_regressor.is_fitted\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"\n",
        "        This function return the decision function values related to each\n",
        "        class on an array of test vectors X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : array of shape [n_samples, n_classes] or [n_samples,]\n",
        "            Decision function values related to each class, per sample.\n",
        "            In the two-class case, the shape is [n_samples,]\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"GenELMClassifier not fitted\")\n",
        "\n",
        "        return self._genelm_regressor.predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"GenELMClassifier not fitted\")\n",
        "\n",
        "        raw_predictions = self.decision_function(X)\n",
        "        class_predictions = self.binarizer.inverse_transform(raw_predictions)\n",
        "\n",
        "        return class_predictions\n",
        "\n",
        "\n",
        "class ELMRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"\n",
        "    Regression model based on Extreme Learning Machine.\n",
        "\n",
        "    An Extreme Learning Machine (ELM) is a single layer feedforward\n",
        "    network with a random hidden layer components and ordinary linear\n",
        "    least squares fitting of the hidden->output weights by default.\n",
        "    [1][2]\n",
        "\n",
        "    ELMRegressor is a wrapper for an GenELMRegressor that creates a\n",
        "    RandomLayer based on the given parameters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `n_hidden` : int, optional (default=20)\n",
        "        Number of units to generate in the SimpleRandomLayer\n",
        "\n",
        "    `alpha` : float, optional (default=0.5)\n",
        "        Mixing coefficient for distance and dot product input activations:\n",
        "        activation = alpha*mlp_activation + (1-alpha)*rbf_width*rbf_activation\n",
        "\n",
        "    `rbf_width` : float, optional (default=1.0)\n",
        "        multiplier on rbf_activation\n",
        "\n",
        "    `activation_func` : {callable, string} optional (default='sigmoid')\n",
        "        Function used to transform input activation\n",
        "\n",
        "        It must be one of 'tanh', 'sine', 'tribas', 'inv_tribas', 'sigmoid',\n",
        "        'hardlim', 'softlim', 'gaussian', 'multiquadric', 'inv_multiquadric' or\n",
        "        a callable.  If none is given, 'tanh' will be used. If a callable\n",
        "        is given, it will be used to compute the hidden unit activations.\n",
        "\n",
        "    `activation_args` : dictionary, optional (default=None)\n",
        "        Supplies keyword arguments for a callable activation_func\n",
        "\n",
        "    `user_components`: dictionary, optional (default=None)\n",
        "        dictionary containing values for components that woud otherwise be\n",
        "        randomly generated.  Valid key/value pairs are as follows:\n",
        "           'radii'  : array-like of shape [n_hidden]\n",
        "           'centers': array-like of shape [n_hidden, n_features]\n",
        "           'biases' : array-like of shape [n_hidden]\n",
        "           'weights': array-like of shape [n_hidden, n_features]\n",
        "\n",
        "    `regressor`    : regressor instance, optional\n",
        "        (default=sklearn.linear_model.LinearRegression())\n",
        "        Used to perform the regression from hidden unit activations\n",
        "        to the outputs and subsequent predictions.\n",
        "\n",
        "    `random_state`  : int, RandomState instance or None (default=None)\n",
        "        Control the pseudo random number generator used to generate the\n",
        "        hidden unit weights at fit time.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `genelm_regressor_` : GenELMRegressor object\n",
        "        Wrapped object that actually performs the fit.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from pyoselm import ELMRegressor\n",
        "    >>> from sklearn.datasets import make_regression\n",
        "    >>> X, y = make_regression(n_samples=100, n_targets=1, n_features=10)\n",
        "    >>> model = ELMRegressor(n_hidden=20,\n",
        "    ...                      activation_func=\"tanh\",\n",
        "    ...                      random_state=123)\n",
        "    >>> model.fit(X, y)\n",
        "    ELMRegressor(random_state=123)\n",
        "    >>> model.score(X, y)\n",
        "    0.8600650083210614\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    GenELMRegressor, RandomLayer, MLPRandomLayer,\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://www.extreme-learning-machines.org\n",
        "    .. [2] G.-B. Huang, Q.-Y. Zhu and C.-K. Siew, \"Extreme Learning Machine:\n",
        "          Theory and Applications\", Neurocomputing, vol. 70, pp. 489-501,\n",
        "              2006.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_hidden=20,\n",
        "                 alpha=0.5,\n",
        "                 rbf_width=1.0,\n",
        "                 activation_func='sigmoid',\n",
        "                 activation_args=None,\n",
        "                 user_components=None,\n",
        "                 regressor=None,\n",
        "                 random_state=None,):\n",
        "\n",
        "        self.n_hidden = n_hidden\n",
        "        self.alpha = alpha\n",
        "        self.random_state = random_state\n",
        "        self.activation_func = activation_func\n",
        "        self.activation_args = activation_args\n",
        "        self.user_components = user_components\n",
        "        self.rbf_width = rbf_width\n",
        "        self.regressor = regressor\n",
        "\n",
        "        # Just to validate input arguments\n",
        "        self._create_random_layer()\n",
        "\n",
        "        self._genelm_regressor = None\n",
        "\n",
        "    def _create_random_layer(self):\n",
        "        \"\"\"Pass init params to RandomLayer\"\"\"\n",
        "\n",
        "        return RandomLayer(n_hidden=self.n_hidden,\n",
        "                           alpha=self.alpha, random_state=self.random_state,\n",
        "                           activation_func=self.activation_func,\n",
        "                           activation_args=self.activation_args,\n",
        "                           user_components=self.user_components,\n",
        "                           rbf_width=self.rbf_width)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "        rhl = self._create_random_layer()\n",
        "        self._genelm_regressor = GenELMRegressor(hidden_layer=rhl,\n",
        "                                                 regressor=self.regressor)\n",
        "        self._genelm_regressor.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def is_fitted(self):\n",
        "        \"\"\"Check if model was fitted\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            boolean, True if model is fitted\n",
        "        \"\"\"\n",
        "        return self._genelm_regressor is not None and self._genelm_regressor.is_fitted\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"ELMRegressor is not fitted\")\n",
        "\n",
        "        return self._genelm_regressor.predict(X)\n",
        "\n",
        "\n",
        "# TODO: inherit from BaseELMClassifier\n",
        "class ELMClassifier(ELMRegressor):\n",
        "    \"\"\"\n",
        "    Classification model based on Extreme Learning Machine.\n",
        "\n",
        "    An Extreme Learning Machine (ELM) is a single layer feedforward\n",
        "    network with a random hidden layer components and ordinary linear\n",
        "    least squares fitting of the hidden->output weights by default.\n",
        "    [1][2]\n",
        "\n",
        "    ELMClassifier is an ELMRegressor subclass that first binarizes the\n",
        "    data, then uses the superclass to compute the decision function that\n",
        "    is then unbinarized to yield the prediction.\n",
        "\n",
        "    The params for the RandomLayer used in the input transform are\n",
        "    exposed in the ELMClassifier constructor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `n_hidden` : int, optional (default=20)\n",
        "        Number of units to generate in the SimpleRandomLayer\n",
        "\n",
        "    `activation_func` : {callable, string} optional (default='sigmoid')\n",
        "        Function used to transform input activation\n",
        "\n",
        "        It must be one of 'tanh', 'sine', 'tribas', 'inv_tribas', 'sigmoid',\n",
        "        'hardlim', 'softlim', 'gaussian', 'multiquadric', 'inv_multiquadric' or\n",
        "        a callable. If a callable is given, it will be used to compute\n",
        "        the hidden unit activations.\n",
        "\n",
        "    `activation_args` : dictionary, optional (default=None)\n",
        "        Supplies keyword arguments for a callable activation_func\n",
        "\n",
        "    `random_state`  : int, RandomState instance or None (default=None)\n",
        "        Control the pseudo random number generator used to generate the\n",
        "        hidden unit weights at fit time.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `classes_` : numpy array of shape [n_classes]\n",
        "        Array of class labels\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from pyoselm import ELMClassifier\n",
        "    >>> from sklearn.datasets import load_digits\n",
        "    >>> X, y = load_digits(n_class=10, return_X_y=True)\n",
        "    >>> model = ELMClassifier(n_hidden=50,\n",
        "    ...                       activation_func=\"sigmoid\",\n",
        "    ...                       random_state=123)\n",
        "    >>> model.fit(X, y)\n",
        "    ELMClassifier(activation_func='sigmoid', n_hidden=50, random_state=123)\n",
        "    >>> model.score(X, y)\n",
        "    0.8241513633834168\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    RandomLayer, RBFRandomLayer, MLPRandomLayer,\n",
        "    GenELMRegressor, GenELMClassifier, ELMClassifier\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://www.extreme-learning-machines.org\n",
        "    .. [2] G.-B. Huang, Q.-Y. Zhu and C.-K. Siew, \"Extreme Learning Machine:\n",
        "          Theory and Applications\", Neurocomputing, vol. 70, pp. 489-501,\n",
        "              2006.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_hidden=20,\n",
        "                 alpha=0.5,\n",
        "                 rbf_width=1.0,\n",
        "                 activation_func='sigmoid',\n",
        "                 activation_args=None,\n",
        "                 user_components=None,\n",
        "                 regressor=None,\n",
        "                 binarizer=LabelBinarizer(neg_label=-1, pos_label=1),\n",
        "                 random_state=None,):\n",
        "\n",
        "        super(ELMClassifier, self).__init__(n_hidden=n_hidden,\n",
        "                                            alpha=alpha,\n",
        "                                            random_state=random_state,\n",
        "                                            activation_func=activation_func,\n",
        "                                            activation_args=activation_args,\n",
        "                                            user_components=user_components,\n",
        "                                            rbf_width=rbf_width,\n",
        "                                            regressor=regressor)\n",
        "\n",
        "        self.classes_ = None\n",
        "        self.binarizer = binarizer\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "        self.classes_ = np.unique(y)\n",
        "\n",
        "        y_bin = self.binarizer.fit_transform(y)\n",
        "        super(ELMClassifier, self).fit(X, y_bin)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"\n",
        "        This function return the decision function values related to each\n",
        "        class on an array of test vectors X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : array of shape [n_samples, n_classes] or [n_samples,]\n",
        "            Decision function values related to each class, per sample.\n",
        "            In the two-class case, the shape is [n_samples,]\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"ELMClassifier is not fitted\")\n",
        "\n",
        "        return super(ELMClassifier, self).predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"ELMClassifier is not fitted\")\n",
        "\n",
        "        raw_predictions = self.decision_function(X)\n",
        "        class_predictions = self.binarizer.inverse_transform(raw_predictions)\n",
        "\n",
        "        return class_predictions\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict probability values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        P : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted probability values.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"ELMClassifier not fitted\")\n",
        "\n",
        "        raw_predictions = self.decision_function(X)\n",
        "        # using softmax to translate raw predictions into probability values\n",
        "        proba_predictions = softmax(raw_predictions)\n",
        "\n",
        "        return proba_predictions\n",
        "\n",
        "    def score(self, X, y, **kwargs):\n",
        "        \"\"\"Force use of accuracy score since\n",
        "        it doesn't inherit from ClassifierMixin\"\"\"\n",
        "        return accuracy_score(y, self.predict(X))"
      ],
      "metadata": {
        "id": "SZbwFpZcJUYj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Module to provide random layers for ELM and OS-ELM models\"\"\"\n",
        "\n",
        "# ===================================================\n",
        "# Acknowledgement:\n",
        "# Author: David C. Lambert [dcl -at- panix -dot- com]\n",
        "# Copyright(c) 2013\n",
        "# License: Simple BSD\n",
        "# ===================================================\n",
        "\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from math import sqrt\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from scipy.spatial.distance import cdist, pdist, squareform\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.extmath import safe_sparse_dot\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    \"RandomLayer\",\n",
        "    \"MLPRandomLayer\",\n",
        "    \"RBFRandomLayer\",\n",
        "    \"GRBFRandomLayer\",\n",
        "]\n",
        "\n",
        "\n",
        "class BaseRandomLayer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Abstract Base class for random layers\"\"\"\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    _internal_activation_funcs = dict()\n",
        "\n",
        "    @classmethod\n",
        "    def activation_func_names(cls):\n",
        "        \"\"\"Get list of internal activation function names\"\"\"\n",
        "        return cls._internal_activation_funcs.keys()\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_hidden=20,\n",
        "                 random_state=0,\n",
        "                 activation_func=None,\n",
        "                 activation_args=None,):\n",
        "\n",
        "        self.n_hidden = n_hidden\n",
        "        self.random_state = random_state\n",
        "        self.activation_func = activation_func\n",
        "        self.activation_args = activation_args\n",
        "\n",
        "        self.components_ = dict()\n",
        "        self.input_activations_ = None\n",
        "\n",
        "        # keyword args for internally defined funcs\n",
        "        self._extra_args = dict()\n",
        "\n",
        "    @abstractmethod\n",
        "    def _generate_components(self, X):\n",
        "        \"\"\"Generate components of hidden layer given X\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def _compute_input_activations(self, X):\n",
        "        \"\"\"Compute input activations given X\"\"\"\n",
        "\n",
        "    def _compute_hidden_activations(self, X):\n",
        "        \"\"\"Compute hidden activations given X\"\"\"\n",
        "        # compute input activations and pass them\n",
        "        # through the hidden layer transfer functions\n",
        "        # to compute the transform\n",
        "\n",
        "        self._compute_input_activations(X)\n",
        "\n",
        "        acts = self.input_activations_\n",
        "\n",
        "        if callable(self.activation_func):\n",
        "            args_dict = self.activation_args if self.activation_args else {}\n",
        "            X_new = self.activation_func(acts, **args_dict)\n",
        "        else:\n",
        "            func_name = self.activation_func\n",
        "            func = self._internal_activation_funcs[func_name]\n",
        "            X_new = func(acts, **self._extra_args)\n",
        "\n",
        "        return X_new\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Generate a random hidden layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training set: only the shape is used to generate random component\n",
        "            values for hidden units\n",
        "\n",
        "        y : not used: placeholder to allow for usage in a Pipeline.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self\n",
        "        \"\"\"\n",
        "        # perform fit by generating random components based\n",
        "        # on the input array\n",
        "        X = check_array(X, accept_sparse=True)\n",
        "        self._generate_components(X)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Generate the random hidden layer's activations given X as input.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
        "            Data to transform\n",
        "\n",
        "        y : not used: placeholder to allow for usage in a Pipeline.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        X_new : numpy array of shape [n_samples, n_components]\n",
        "        \"\"\"\n",
        "        # perform transformation by calling compute_hidden_activations\n",
        "        # (which will normally call compute_input_activations first)\n",
        "        X = check_array(X, accept_sparse=True)\n",
        "\n",
        "        if len(self.components_) == 0:\n",
        "            raise ValueError('No components initialized')\n",
        "\n",
        "        return self._compute_hidden_activations(X)\n",
        "\n",
        "\n",
        "class RandomLayer(BaseRandomLayer):\n",
        "    \"\"\"RandomLayer is a transformer that creates a feature mapping of the\n",
        "    inputs that corresponds to a layer of hidden units with randomly\n",
        "    generated components.\n",
        "\n",
        "    The transformed values are a specified function of input activations\n",
        "    that are a weighted combination of dot product (multilayer perceptron)\n",
        "    and distance (rbf) activations:\n",
        "\n",
        "      input_activation = alpha * mlp_activation + (1-alpha) * rbf_activation\n",
        "\n",
        "      mlp_activation(x) = dot(x, weights) + bias\n",
        "      rbf_activation(x) = rbf_width * ||x - center||/radius\n",
        "\n",
        "      alpha and rbf_width are specified by the user\n",
        "\n",
        "      weights and biases are taken from normal distribution of\n",
        "      mean 0 and sd of 1\n",
        "\n",
        "      centers are taken uniformly from the bounding hyperrectangle\n",
        "      of the inputs, and radii are max(||x-c||)/sqrt(n_centers*2)\n",
        "\n",
        "    The input activation is transformed by a transfer function that defaults\n",
        "    to numpy.tanh if not specified, but can be any callable that returns an\n",
        "    array of the same shape as its argument (the input activation array, of\n",
        "    shape [n_samples, n_hidden]).  Functions provided are 'sine', 'tanh',\n",
        "    'tribas', 'inv_tribas', 'sigmoid', 'hardlim', 'softlim', 'gaussian',\n",
        "    'multiquadric', or 'inv_multiquadric'.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `n_hidden` : int, optional (default=20)\n",
        "        Number of units to generate\n",
        "\n",
        "    `alpha` : float, optional (default=0.5)\n",
        "        Mixing coefficient for distance and dot product input activations:\n",
        "        activation = alpha*mlp_activation + (1-alpha)*rbf_width*rbf_activation\n",
        "\n",
        "    `rbf_width` : float, optional (default=1.0)\n",
        "        multiplier on rbf_activation\n",
        "\n",
        "    `user_components`: dictionary, optional (default=None)\n",
        "        dictionary containing values for components that would otherwise be\n",
        "        randomly generated.  Valid key/value pairs are as follows:\n",
        "           'radii'  : array-like of shape [n_hidden]\n",
        "           'centers': array-like of shape [n_hidden, n_features]\n",
        "           'biases' : array-like of shape [n_hidden]\n",
        "           'weights': array-like of shape [n_features, n_hidden]\n",
        "\n",
        "    `activation_func` : {callable, string} optional (default='tanh')\n",
        "        Function used to transform input activation\n",
        "\n",
        "        It must be one of 'tanh', 'sine', 'tribas', 'inv_tribas',\n",
        "        'sigmoid', 'hardlim', 'softlim', 'gaussian', 'multiquadric',\n",
        "        'inv_multiquadric' or a callable.  If None is given, 'tanh'\n",
        "        will be used.\n",
        "\n",
        "        If a callable is given, it will be used to compute the activations.\n",
        "\n",
        "    `activation_args` : dictionary, optional (default=None)\n",
        "        Supplies keyword arguments for a callable activation_func\n",
        "\n",
        "    `random_state`  : int, RandomState instance or None (default=None)\n",
        "        Control the pseudo random number generator used to generate the\n",
        "        hidden unit weights at fit time.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `input_activations_` : numpy array of shape [n_samples, n_hidden]\n",
        "        Array containing dot(x, hidden_weights) + bias for all samples\n",
        "\n",
        "    `components_` : dictionary containing two keys:\n",
        "        `bias_weights_`   : numpy array of shape [n_hidden]\n",
        "        `hidden_weights_` : numpy array of shape [n_features, n_hidden]\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    \"\"\"\n",
        "\n",
        "    # triangular activation function\n",
        "    _tribas = lambda x: np.clip(1.0 - np.fabs(x), 0.0, 1.0)\n",
        "\n",
        "    # inverse triangular activation function\n",
        "    _inv_tribas = lambda x: np.clip(np.fabs(x), 0.0, 1.0)\n",
        "\n",
        "    # sigmoid activation function\n",
        "    _sigmoid = lambda x: 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "    # hard limit activation function\n",
        "    _hardlim = lambda x: np.array(x > 0.0, dtype=float)\n",
        "\n",
        "    _softlim = lambda x: np.clip(x, 0.0, 1.0)\n",
        "\n",
        "    # identity or linear activation function\n",
        "    _linear = lambda x: x\n",
        "\n",
        "    # ReLU\n",
        "    _relu = lambda x: np.maximum(x, 0)\n",
        "\n",
        "    # Softplus activation function\n",
        "    _softplus = lambda x: np.log(1.0 + np.exp(x))\n",
        "\n",
        "    # gaussian RBF\n",
        "    _gaussian = lambda x: np.exp(-pow(x, 2.0))\n",
        "\n",
        "    # multiquadric RBF\n",
        "    _multiquadric = lambda x: np.sqrt(1.0 + pow(x, 2.0))\n",
        "\n",
        "    # inverse multiquadric RBF\n",
        "    _inv_multiquadric = lambda x: 1.0/(np.sqrt(1.0 + pow(x, 2.0)))\n",
        "\n",
        "    # internal activation function table\n",
        "    _internal_activation_funcs = {\n",
        "        'sine': np.sin,\n",
        "        'tanh': np.tanh,\n",
        "        'tribas': _tribas,\n",
        "        'inv_tribas': _inv_tribas,\n",
        "        'linear': _linear,\n",
        "        'relu': _relu,\n",
        "        'softplus': _softplus,\n",
        "        'sigmoid': _sigmoid,\n",
        "        'softlim': _softlim,\n",
        "        'hardlim': _hardlim,\n",
        "        'gaussian': _gaussian,\n",
        "        'multiquadric': _multiquadric,\n",
        "        'inv_multiquadric': _inv_multiquadric,\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_hidden=20,\n",
        "                 alpha=0.5,\n",
        "                 random_state=None,\n",
        "                 activation_func='tanh',\n",
        "                 activation_args=None,\n",
        "                 user_components=None,\n",
        "                 rbf_width=1.0,):\n",
        "\n",
        "        super(RandomLayer, self).__init__(\n",
        "            n_hidden=n_hidden,\n",
        "            random_state=random_state,\n",
        "            activation_func=activation_func,\n",
        "            activation_args=activation_args\n",
        "        )\n",
        "\n",
        "        if isinstance(self.activation_func, str):\n",
        "            func_names = self._internal_activation_funcs.keys()\n",
        "            if self.activation_func not in func_names:\n",
        "                msg = \"Unknown activation function '%s'\" % self.activation_func\n",
        "                raise ValueError(msg)\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.rbf_width = rbf_width\n",
        "        self.user_components = user_components\n",
        "\n",
        "        self._use_mlp_input = (self.alpha != 0.0)\n",
        "        self._use_rbf_input = (self.alpha != 1.0)\n",
        "\n",
        "    def _get_user_components(self, key):\n",
        "        \"\"\"Look for given user component\"\"\"\n",
        "        try:\n",
        "            return self.user_components[key]\n",
        "        except (TypeError, KeyError):\n",
        "            return None\n",
        "\n",
        "    def _compute_radii(self):\n",
        "        \"\"\"Generate RBF radii\"\"\"\n",
        "\n",
        "        # use supplied radii if present\n",
        "        radii = self._get_user_components('radii')\n",
        "\n",
        "        # compute radii\n",
        "        if radii is None:\n",
        "            centers = self.components_['centers']\n",
        "\n",
        "            n_centers = centers.shape[0]\n",
        "            max_dist = np.max(pairwise_distances(centers))\n",
        "            radii = np.ones(n_centers) * max_dist/sqrt(2.0 * n_centers)\n",
        "\n",
        "        self.components_['radii'] = radii\n",
        "\n",
        "    def _compute_centers(self, X, sparse, rs):\n",
        "        \"\"\"Generate RBF centers\"\"\"\n",
        "\n",
        "        # use supplied centers if present\n",
        "        centers = self._get_user_components('centers')\n",
        "\n",
        "        # use points taken uniformly from the bounding hyperrectangle\n",
        "        if centers is None:\n",
        "            n_features = X.shape[1]\n",
        "\n",
        "            if sparse:\n",
        "                cols = [X.getcol(i) for i in range(n_features)]\n",
        "\n",
        "                min_dtype = X.dtype.type(1.0e10)\n",
        "                sp_min = lambda col: np.minimum(min_dtype, np.min(col.data))\n",
        "                min_Xs = np.array(list(map(sp_min, cols)))\n",
        "\n",
        "                max_dtype = X.dtype.type(-1.0e10)\n",
        "                sp_max = lambda col: np.maximum(max_dtype, np.max(col.data))\n",
        "                max_Xs = np.array(list(map(sp_max, cols)))\n",
        "            else:\n",
        "                min_Xs = X.min(axis=0)\n",
        "                max_Xs = X.max(axis=0)\n",
        "\n",
        "            spans = max_Xs - min_Xs\n",
        "            ctrs_size = (self.n_hidden, n_features)\n",
        "            centers = min_Xs + spans * rs.uniform(0.0, 1.0, ctrs_size)\n",
        "\n",
        "        self.components_['centers'] = centers\n",
        "\n",
        "    def _compute_biases(self, rs):\n",
        "        \"\"\"Generate MLP biases\"\"\"\n",
        "\n",
        "        # use supplied biases if present\n",
        "        biases = self._get_user_components('biases')\n",
        "        if biases is None:\n",
        "            b_size = self.n_hidden\n",
        "            biases = rs.normal(size=b_size)\n",
        "\n",
        "        self.components_['biases'] = biases\n",
        "\n",
        "    def _compute_weights(self, X, rs):\n",
        "        \"\"\"Generate MLP weights\"\"\"\n",
        "\n",
        "        # use supplied weights if present\n",
        "        weights = self._get_user_components('weights')\n",
        "        if weights is None:\n",
        "            n_features = X.shape[1]\n",
        "            hw_size = (n_features, self.n_hidden)\n",
        "            weights = rs.normal(size=hw_size)\n",
        "\n",
        "        self.components_['weights'] = weights\n",
        "\n",
        "    def _generate_components(self, X):\n",
        "        \"\"\"Generate components of hidden layer given X\"\"\"\n",
        "\n",
        "        rs = check_random_state(self.random_state)\n",
        "        if self._use_mlp_input:\n",
        "            self._compute_biases(rs)\n",
        "            self._compute_weights(X, rs)\n",
        "\n",
        "        if self._use_rbf_input:\n",
        "            self._compute_centers(X, sp.issparse(X), rs)\n",
        "            self._compute_radii()\n",
        "\n",
        "    def _compute_input_activations(self, X):\n",
        "        \"\"\"Compute input activations given X\"\"\"\n",
        "\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        mlp_acts = np.zeros((n_samples, self.n_hidden))\n",
        "        if self._use_mlp_input:\n",
        "            b = self.components_['biases']\n",
        "            w = self.components_['weights']\n",
        "            mlp_acts = self.alpha * (safe_sparse_dot(X, w) + b)\n",
        "\n",
        "        rbf_acts = np.zeros((n_samples, self.n_hidden))\n",
        "        if self._use_rbf_input:\n",
        "            radii = self.components_['radii']\n",
        "            centers = self.components_['centers']\n",
        "            scale = self.rbf_width * (1.0 - self.alpha)\n",
        "\n",
        "            if sp.issparse(X):\n",
        "                X = X.todense()\n",
        "\n",
        "            rbf_acts = scale * cdist(X, centers)/radii\n",
        "\n",
        "        self.input_activations_ = mlp_acts + rbf_acts\n",
        "\n",
        "\n",
        "class MLPRandomLayer(RandomLayer):\n",
        "    \"\"\"Wrapper for RandomLayer with alpha (mixing coefficient) set\n",
        "       to 1.0 for MLP activations only\"\"\"\n",
        "\n",
        "    def __init__(self, n_hidden=20, random_state=None,\n",
        "                 activation_func='tanh', activation_args=None,\n",
        "                 weights=None, biases=None):\n",
        "\n",
        "        user_components = {'weights': weights, 'biases': biases}\n",
        "        super(MLPRandomLayer, self).__init__(\n",
        "            n_hidden=n_hidden,\n",
        "            random_state=random_state,\n",
        "            activation_func=activation_func,\n",
        "            activation_args=activation_args,\n",
        "            user_components=user_components,\n",
        "            alpha=1.0\n",
        "        )\n",
        "\n",
        "\n",
        "class RBFRandomLayer(RandomLayer):\n",
        "    \"\"\"Wrapper for RandomLayer with alpha (mixing coefficient) set\n",
        "       to 0.0 for RBF activations only\"\"\"\n",
        "\n",
        "    def __init__(self, n_hidden=20, random_state=None,\n",
        "                 activation_func='gaussian', activation_args=None,\n",
        "                 centers=None, radii=None, rbf_width=1.0):\n",
        "\n",
        "        user_components = {'centers': centers, 'radii': radii}\n",
        "        super(RBFRandomLayer, self).__init__(\n",
        "            n_hidden=n_hidden,\n",
        "            random_state=random_state,\n",
        "            activation_func=activation_func,\n",
        "            activation_args=activation_args,\n",
        "            user_components=user_components,\n",
        "            rbf_width=rbf_width,\n",
        "            alpha=0.0\n",
        "        )\n",
        "\n",
        "\n",
        "class GRBFRandomLayer(RBFRandomLayer):\n",
        "    \"\"\"Random Generalized RBF Hidden Layer transformer\n",
        "\n",
        "    Creates a layer of radial basis function units where:\n",
        "\n",
        "       f(a), s.t. a = ||x-c||/r\n",
        "\n",
        "    with c the unit center\n",
        "    and f() is exp(-gamma * a^tau) where tau and r are computed\n",
        "    based on [1]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `n_hidden` : int, optional (default=20)\n",
        "        Number of units to generate, ignored if centers are provided\n",
        "\n",
        "    `grbf_lambda` : float, optional (default=0.05)\n",
        "        GRBF shape parameter\n",
        "\n",
        "    `gamma` : {int, float} optional (default=1.0)\n",
        "        Width multiplier for GRBF distance argument\n",
        "\n",
        "    `centers` : array of shape (n_hidden, n_features), optional (default=None)\n",
        "        If provided, overrides internal computation of the centers\n",
        "\n",
        "    `radii` : array of shape (n_hidden),  optional (default=None)\n",
        "        If provided, overrides internal computation of the radii\n",
        "\n",
        "    `use_exemplars` : bool, optional (default=False)\n",
        "        If True, uses random examples from the input to determine the RBF\n",
        "        centers, ignored if centers are provided\n",
        "\n",
        "    `random_state`  : int or RandomState instance, optional (default=None)\n",
        "        Control the pseudo random number generator used to generate the\n",
        "        centers at fit time, ignored if centers are provided\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `components_` : dictionary containing two keys:\n",
        "        `radii_`   : numpy array of shape [n_hidden]\n",
        "        `centers_` : numpy array of shape [n_hidden, n_features]\n",
        "\n",
        "    `input_activations_` : numpy array of shape [n_samples, n_hidden]\n",
        "        Array containing ||x-c||/r for all samples\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    ELMRegressor, ELMClassifier, SimpleELMRegressor, SimpleELMClassifier,\n",
        "    SimpleRandomLayer\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Fernandez-Navarro, et al, \"MELM-GRBF: a modified version of the\n",
        "              extreme learning machine for generalized radial basis function\n",
        "              neural networks\", Neurocomputing 74 (2011), 2502-2510\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_hidden=20, grbf_lambda=0.001,\n",
        "                 centers=None, radii=None, random_state=None):\n",
        "\n",
        "        self._internal_activation_funcs = {'grbf': self._grbf}\n",
        "\n",
        "        super(GRBFRandomLayer, self).__init__(\n",
        "            n_hidden=n_hidden,\n",
        "            activation_func='grbf',\n",
        "            centers=centers, radii=radii,\n",
        "            random_state=random_state\n",
        "        )\n",
        "\n",
        "        self.grbf_lambda = grbf_lambda\n",
        "        self.dN_vals = None\n",
        "        self.dF_vals = None\n",
        "        self.tau_vals = None\n",
        "\n",
        "    @staticmethod\n",
        "    def _grbf(acts, taus):\n",
        "        \"\"\"GRBF activation function\"\"\"\n",
        "        return np.exp(np.exp(-pow(acts, taus)))\n",
        "\n",
        "    def _compute_centers(self, X, sparse, rs):\n",
        "        \"\"\"Generate centers, then compute tau, dF and dN vals\"\"\"\n",
        "        # get centers from superclass, then calculate tau_vals\n",
        "        # according to ref [1]\n",
        "        super(GRBFRandomLayer, self)._compute_centers(X, sparse, rs)\n",
        "\n",
        "        centers = self.components_['centers']\n",
        "        sorted_distances = np.sort(squareform(pdist(centers)))\n",
        "        self.dF_vals = sorted_distances[:, -1]\n",
        "        self.dN_vals = sorted_distances[:, 1]/100.0\n",
        "\n",
        "        tauNum = np.log(np.log(self.grbf_lambda) /\n",
        "                        np.log(1.0 - self.grbf_lambda))\n",
        "\n",
        "        tauDenom = np.log(self.dF_vals/self.dN_vals)\n",
        "\n",
        "        self.tau_vals = tauNum/tauDenom\n",
        "\n",
        "        self._extra_args['taus'] = self.tau_vals\n",
        "\n",
        "    def _compute_radii(self):\n",
        "        \"\"\"Generate radii\"\"\"\n",
        "        # according to ref [1]\n",
        "        denom = pow(-np.log(self.grbf_lambda), 1.0/self.tau_vals)\n",
        "        self.components_['radii'] = self.dF_vals/denom"
      ],
      "metadata": {
        "id": "3ueVkazcJaTZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Module to build Online Sequential Extreme Learning Machine (OS-ELM) models\"\"\"\n",
        "\n",
        "# ===================================================\n",
        "# Author: Leandro Ferrado\n",
        "# Copyright(c) 2018\n",
        "# License: Apache License 2.0\n",
        "# ===================================================\n",
        "\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "from scipy.linalg import pinv\n",
        "from scipy.sparse import eye\n",
        "from scipy.special import softmax\n",
        "from sklearn.base import RegressorMixin, BaseEstimator\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.utils import as_float_array\n",
        "from sklearn.utils.extmath import safe_sparse_dot\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    \"OSELMRegressor\",\n",
        "    \"OSELMClassifier\",\n",
        "]\n",
        "\n",
        "\n",
        "def multiple_safe_sparse_dot(*matrices):\n",
        "    \"\"\"\n",
        "    Make safe_sparse_dot() calls over multiple matrices\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    matrices: iterable of matrices\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dot_product : array or sparse matrix\n",
        "    \"\"\"\n",
        "    if len(matrices) < 2:\n",
        "        raise ValueError(\"Argument 'matrices' must have at least 2 matrices\")\n",
        "\n",
        "    r = matrices[0]\n",
        "    for m in matrices[1:]:\n",
        "        r = safe_sparse_dot(r, m)\n",
        "\n",
        "    return r\n",
        "\n",
        "\n",
        "class OSELMRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"\n",
        "    OSELMRegressor is a regressor based on Online Sequential\n",
        "    Extreme Learning Machine (OS-ELM).\n",
        "\n",
        "    This type of model is an ELM that....   ...\n",
        "    [1][2]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `n_hidden` : int, optional (default=20)\n",
        "        Number of units to generate in the SimpleRandomLayer\n",
        "\n",
        "    `activation_func` : {callable, string} optional (default='sigmoid')\n",
        "        Function used to transform input activation\n",
        "\n",
        "        It must be one of 'tanh', 'sine', 'tribas', 'inv_tribas', 'sigmoid',\n",
        "        'hardlim', 'softlim', 'gaussian', 'multiquadric', 'inv_multiquadric' or\n",
        "        a callable.  If none is given, 'tanh' will be used. If a callable\n",
        "        is given, it will be used to compute the hidden unit activations.\n",
        "\n",
        "    `activation_args` : dictionary, optional (default=None)\n",
        "        Supplies keyword arguments for a callable activation_func\n",
        "\n",
        "    `use_woodbury`  : bool, optional (default=False)\n",
        "        Flag to indicate if Woodbury formula should be used for the fit\n",
        "        step, or just the traditional iterative procedure. Not recommended if\n",
        "        handling large datasets.\n",
        "\n",
        "    `random_state`  : int, RandomState instance or None (default=None)\n",
        "        Control the pseudo random number generator used to generate the\n",
        "        hidden unit weights at fit time.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `P` : np.array\n",
        "        ...\n",
        "\n",
        "    `beta` : np.array\n",
        "    ...\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    ELMRegressor, MLPRandomLayer\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://www.extreme-learning-machines.org\n",
        "    .. [2] G.-B. Huang, Q.-Y. Zhu and C.-K. Siew, \"Extreme Learning Machine:\n",
        "          Theory and Applications\", Neurocomputing, vol. 70, pp. 489-501,\n",
        "              2006.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_hidden=20,\n",
        "                 activation_func='sigmoid',\n",
        "                 activation_args=None,\n",
        "                 use_woodbury=False,\n",
        "                 random_state=123,):\n",
        "        self.n_hidden = n_hidden\n",
        "        self.random_state = random_state\n",
        "        self.activation_func = activation_func\n",
        "        self.activation_args = activation_args\n",
        "        self.use_woodbury = use_woodbury\n",
        "\n",
        "        self.P = None\n",
        "        self.beta = None\n",
        "\n",
        "    def _create_random_layer(self):\n",
        "        \"\"\"Pass init params to MLPRandomLayer\"\"\"\n",
        "\n",
        "        return MLPRandomLayer(n_hidden=self.n_hidden,\n",
        "                              random_state=self.random_state,\n",
        "                              activation_func=self.activation_func,\n",
        "                              activation_args=self.activation_args)\n",
        "\n",
        "    def _fit_woodbury(self, X, y):\n",
        "        \"\"\"Compute learning step using Woodbury formula\"\"\"\n",
        "        # fit random hidden layer and compute the hidden layer activations\n",
        "        H = self._create_random_layer().fit_transform(X)\n",
        "        y = as_float_array(y, copy=True)\n",
        "\n",
        "        if self.beta is None:\n",
        "            # this is the first time the model is fitted\n",
        "            if len(X) < self.n_hidden:\n",
        "                raise ValueError(\"The first time the model is fitted, \"\n",
        "                                 \"X must have at least equal number of \"\n",
        "                                 \"samples than n_hidden value!\")\n",
        "            self.P = pinv(safe_sparse_dot(H.T, H))\n",
        "            self.beta = multiple_safe_sparse_dot(self.P, H.T, y)\n",
        "        else:\n",
        "            if len(H) > 10e3:\n",
        "                warnings.warn(\"Large input of %i rows and use_woodbury=True \"\\\n",
        "                              \"may throw OOM errors\" % len(H))\n",
        "\n",
        "            M = eye(len(H)) + multiple_safe_sparse_dot(H, self.P, H.T)\n",
        "            self.P -= multiple_safe_sparse_dot(self.P, H.T, pinv(M), H, self.P)\n",
        "            e = y - safe_sparse_dot(H, self.beta)\n",
        "            self.beta += multiple_safe_sparse_dot(self.P, H.T, e)\n",
        "\n",
        "    def _fit_iterative(self, X, y):\n",
        "        \"\"\"Compute learning step using iterative procedure\"\"\"\n",
        "        # fit random hidden layer and compute the hidden layer activations\n",
        "        H = self._create_random_layer().fit_transform(X)\n",
        "        y = as_float_array(y, copy=True)\n",
        "\n",
        "        if self.beta is None:\n",
        "            # this is the first time the model is fitted\n",
        "            print(\"Batch learning...\\n\")\n",
        "            if len(X) < self.n_hidden:\n",
        "                raise ValueError(\"The first time the model is fitted, \"\n",
        "                                 \"X must have at least equal number of \"\n",
        "                                 \"samples than n_hidden value!\")\n",
        "\n",
        "            self.P = safe_sparse_dot(H.T, H)\n",
        "            P_inv = pinv(self.P)\n",
        "            self.beta = multiple_safe_sparse_dot(P_inv, H.T, y)\n",
        "        else:\n",
        "            print(\"Online learning...\\n\")\n",
        "            self.P += safe_sparse_dot(H.T, H)\n",
        "            P_inv = pinv(self.P)\n",
        "            e = y - safe_sparse_dot(H, self.beta)\n",
        "            self.beta = self.beta + multiple_safe_sparse_dot(P_inv, H.T, e)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data.\n",
        "\n",
        "        Notice that this function could be used for n_samples==1 (online learning),\n",
        "        except for the first time the model is fitted, where it needs at least as\n",
        "        many rows as 'n_hidden' configured.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "        if self.use_woodbury:\n",
        "            self._fit_woodbury(X, y)\n",
        "        else:\n",
        "            self._fit_iterative(X, y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def partial_fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data. Alias for fit() method.\n",
        "\n",
        "        Notice that this function could be used for n_samples==1 (online learning),\n",
        "        except for the first time the model is fitted, where it needs at least as\n",
        "        many rows as 'n_hidden' configured.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "        return self.fit(X, y)\n",
        "\n",
        "    @property\n",
        "    def is_fitted(self):\n",
        "        \"\"\"Check if model was fitted\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            boolean, True if model is fitted\n",
        "        \"\"\"\n",
        "        return self.beta is not None\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"OSELMRegressor not fitted\")\n",
        "\n",
        "        # compute hidden layer activations\n",
        "        H = self._create_random_layer().fit_transform(X)\n",
        "\n",
        "        # compute output predictions for new hidden activations\n",
        "        predictions = safe_sparse_dot(H, self.beta)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "class OSELMClassifier(OSELMRegressor):\n",
        "    \"\"\"\n",
        "    OSELMClassifier is a classifier based on the Extreme Learning Machine.\n",
        "\n",
        "    An Extreme Learning Machine (ELM) is a single layer feedforward\n",
        "    network with a random hidden layer components and ordinary linear\n",
        "    least squares fitting of the hidden->output weights by default.\n",
        "    [1][2]\n",
        "\n",
        "    OSELMClassifier is an OSELMRegressor subclass that first binarizes the\n",
        "    data, then uses the superclass to compute the decision function that\n",
        "    is then unbinarized to yield the prediction.\n",
        "\n",
        "    The params for the RandomLayer used in the input transform are\n",
        "    exposed in the ELMClassifier constructor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `n_hidden` : int, optional (default=20)\n",
        "        Number of units to generate in the SimpleRandomLayer\n",
        "\n",
        "    `activation_func` : {callable, string} optional (default='sigmoid')\n",
        "        Function used to transform input activation\n",
        "\n",
        "        It must be one of 'tanh', 'sine', 'tribas', 'inv_tribas', 'sigmoid',\n",
        "        'hardlim', 'softlim', 'gaussian', 'multiquadric', 'inv_multiquadric' or\n",
        "        a callable.  If none is given, 'tanh' will be used. If a callable\n",
        "        is given, it will be used to compute the hidden unit activations.\n",
        "\n",
        "    `activation_args` : dictionary, optional (default=None)\n",
        "        Supplies keyword arguments for a callable activation_func\n",
        "\n",
        "    `random_state`  : int, RandomState instance or None (default=None)\n",
        "        Control the pseudo random number generator used to generate the\n",
        "        hidden unit weights at fit time.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `classes_` : numpy array of shape [n_classes]\n",
        "        Array of class labels\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    ELMRegressor, OSELMRegressor, MLPRandomLayer\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://www.extreme-learning-machines.org\n",
        "    .. [2] G.-B. Huang, Q.-Y. Zhu and C.-K. Siew, \"Extreme Learning Machine:\n",
        "          Theory and Applications\", Neurocomputing, vol. 70, pp. 489-501,\n",
        "              2006.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_hidden=20,\n",
        "                 activation_func='sigmoid',\n",
        "                 activation_args=None,\n",
        "                 binarizer=LabelBinarizer(neg_label=-1, pos_label=1),\n",
        "                 use_woodbury=False,\n",
        "                 random_state=123):\n",
        "\n",
        "        super(OSELMClassifier, self).__init__(n_hidden=n_hidden,\n",
        "                                              random_state=random_state,\n",
        "                                              activation_func=activation_func,\n",
        "                                              activation_args=activation_args,\n",
        "                                              use_woodbury=use_woodbury)\n",
        "        self.classes_ = None\n",
        "        self.binarizer = binarizer\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"\n",
        "        This function return the decision function values related to each\n",
        "        class on an array of test vectors X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : array of shape [n_samples, n_classes] or [n_samples,]\n",
        "            Decision function values related to each class, per sample.\n",
        "            In the two-class case, the shape is [n_samples,]\n",
        "        \"\"\"\n",
        "        return super(OSELMClassifier, self).predict(X)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            self.classes_ = np.unique(y)\n",
        "            y_bin = self.binarizer.fit_transform(y)\n",
        "        else:\n",
        "            y_bin = self.binarizer.transform(y)\n",
        "\n",
        "        super(OSELMClassifier, self).fit(X, y_bin)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted class values.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"OSELMClassifier not fitted\")\n",
        "\n",
        "        raw_predictions = self.decision_function(X)\n",
        "        class_predictions = self.binarizer.inverse_transform(raw_predictions)\n",
        "\n",
        "        return class_predictions\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict probability values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        P : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted probability values.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"OSELMClassifier not fitted\")\n",
        "\n",
        "        raw_predictions = self.decision_function(X)\n",
        "        # using softmax to translate raw predictions into probability values\n",
        "        proba_predictions = softmax(raw_predictions)\n",
        "\n",
        "        return proba_predictions\n",
        "\n",
        "    def score(self, X, y, **kwargs):\n",
        "        \"\"\"Force use of accuracy score since\n",
        "        it doesn't inherit from ClassifierMixin\"\"\"\n",
        "        return accuracy_score(y, self.predict(X))"
      ],
      "metadata": {
        "id": "CZWVvm8kLcHq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hHYT6eqn6F0J"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# hide code cell output\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "abkESiyPrulD"
      },
      "outputs": [],
      "source": [
        "# from elm import ELMRegressor\n",
        "from collections.abc import Iterable, Generator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as py\n",
        "import random\n",
        "import scipy.stats as stats\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "# from skmultiflow.drift_detection import DDM\n",
        "# from river import drift\n",
        "import statsmodels.api as sm\n",
        "import sys\n",
        "from typing import Union, Optional, Tuple, List\n",
        "# for input x (multi features)\n",
        "multi_sc = preprocessing.StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Rg0f2bme4Ise"
      },
      "outputs": [],
      "source": [
        "def train_time_series(train_split: int = 5, test_size: Optional[int] = 250, **kwargs) -> Generator:\n",
        "    # sudden drift = sudden-2000-06-13-19.csv, fake-2000-2024-06-13-16, sudden-2000-06-22-16\n",
        "    filename = \"/content/drive/MyDrive/Colab Notebooks/fake-2000-2024-06-13-16.csv\"\n",
        "    df = pd.read_csv(filename, parse_dates=['Date'], index_col='Date')\n",
        "    df= df.sort_values(by='Date')\n",
        "    df = df.rename(columns={\"Close/Last\": \"close\"})\n",
        "    df.columns = df.columns.str.lower()\n",
        "    # replace Nan\n",
        "    df['volume'] = df['volume'].interpolate()\n",
        "\n",
        "    # 1) lowest and highest prices of the 5 previous days;\n",
        "    df['high_previous_1_days'] = df['high'].shift(1)\n",
        "    df['low_previous_1_days'] = df['low'].shift(1)\n",
        "\n",
        "\n",
        "    # 2) opening and closing prices of the 5 previous days;\n",
        "    df['close_previous_1_days'] = df['close'].shift(1)\n",
        "    df['open_previous_1_days'] = df['open'].shift(1)\n",
        "\n",
        "    # change = yesterday - the day before yesterday\n",
        "    df['change'] = df['close'].diff()\n",
        "    df['change'] = df['change'].shift(1)\n",
        "\n",
        "    # exclude 5 columns, others = input features, start from index=5 drop NaN\n",
        "    input_X = df[df.columns.difference(['close', 'open', 'high', 'low'])][2:]\n",
        "    input_y = df[\"close\"][2:]\n",
        "    # 2d-array x.shape = (input_samples, features)\n",
        "    X = input_X.to_numpy()\n",
        "    y = input_y.to_numpy()\n",
        "    # split list in half, first part: training data, last part: streaming data\n",
        "    X_last = X[len(X)//2:]\n",
        "    X = X[:len(X)//2]\n",
        "    y_last = y[len(y)//2:]\n",
        "    y = y[:len(y)//2]\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=train_split, test_size=test_size)\n",
        "    for train_index, test_index in tscv.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        print(f\"Number of samples for train {len(train_index)} and for test {len(test_index)}\")\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        X_train = X_train.reshape(-1, X.shape[1])\n",
        "        X_train_scaled = multi_sc.fit_transform(X_train)\n",
        "        X_test = X_test.reshape(-1, X.shape[1])\n",
        "        X_test_scaled = multi_sc.transform(X_test)\n",
        "        # assume y is single column\n",
        "        y_train = y_train.reshape(-1, 1)\n",
        "        y_test = y_test.reshape(-1, 1)\n",
        "        nodes = kwargs[\"nodes\"]\n",
        "        random_state = kwargs[\"random_state\"]\n",
        "        oselmr = OSELMRegressor(n_hidden=nodes, random_state=random_state)\n",
        "        x_train, y_train, x_test, y_test = X_train_scaled, y_train, X_test_scaled, y_test\n",
        "        oselmr = oselmr.fit(x_train, y_train)\n",
        "        y_pred = oselmr.predict(x_test)\n",
        "        # squared=True -> MSE\n",
        "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "        # Residual = actual y - predicted y\n",
        "        residual = y_test - y_pred\n",
        "        res_avg = np.mean(residual)\n",
        "        res_std = np.std(residual)\n",
        "        r2 = r2_score(y_true=y_test, y_pred=y_pred)\n",
        "        # n: observations, p: predictors(features)\n",
        "        n = np.shape(x_test)[0]\n",
        "        p = np.shape(x_test)[1]\n",
        "        adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
        "        if r2>0:\n",
        "            print(f\"Hidden nodes: {nodes}, Random state = {random_state}, RMSE = {rmse}\")\n",
        "            print(f\"Residual AVG = {res_avg}, Residual STD = {res_std}\")\n",
        "            print(f\"MAPE = {mean_squared_error(y_test, y_pred)}\")\n",
        "            print(f\"R2 score = {r2}, Adjusted R2 score {adj_r2}\")\n",
        "        yield {\"residual\":residual, \"RMSE\":rmse, \"X_streaming\":X_last, \"y_streaming\":y_last, \"model\":oselmr}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N_rPVkkY6kU4"
      },
      "outputs": [],
      "source": [
        "def count_items_in_interval(input_list: np.ndarray, lower_bound: np.float64, upper_bound: np.float64):\n",
        "    count = len([item for item in input_list if lower_bound <= item <= upper_bound])\n",
        "    return count/len(input_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train model by first half\n",
        "def train_part(nodes:int = 100):\n",
        "    # model parameters\n",
        "    train_split = 2\n",
        "    test_size = 100\n",
        "\n",
        "    ''' Test Number of Nodes '''\n",
        "    times = 1\n",
        "    random_list = random.sample(range(0, 2**32 - 1), times)\n",
        "\n",
        "    all_rmse = []\n",
        "    all_residual = np.array([])\n",
        "    X_streaming = []\n",
        "    y_streaming = []\n",
        "    model = OSELMRegressor()\n",
        "\n",
        "    ''' Test Number of Nodes '''\n",
        "    for random_num in random_list:\n",
        "        print(\"Random number \",random_num)\n",
        "        results = train_time_series(train_split=train_split, test_size=test_size, nodes=nodes, random_state=random_num)\n",
        "        for res in results:\n",
        "            X_streaming = res[\"X_streaming\"]\n",
        "            y_streaming = res[\"y_streaming\"]\n",
        "            model = res[\"model\"]\n",
        "            all_rmse.append(res[\"RMSE\"])\n",
        "            all_residual = np.append(all_residual, res[\"residual\"])\n",
        "\n",
        "    plt.clf()\n",
        "    if times!=1:\n",
        "        plt.hist(all_rmse, bins=8, range=(0, 40), facecolor=\"red\", edgecolor=\"black\", alpha=0.3)\n",
        "        plt.xlabel(\"RMSE\")\n",
        "        plt.ylabel(\"frequency\")\n",
        "        plt.show(block=False)\n",
        "    rmse_avg = np.mean(all_rmse)\n",
        "    rmse_std = np.std(all_rmse)\n",
        "    print(f\"RMSE AVG = {rmse_avg}, RMSE STD = {rmse_std}\")\n",
        "    res_avg = np.mean(all_residual)\n",
        "    res_std = np.std(all_residual)\n",
        "    print(f\"Residual AVG = {res_avg}, Residual STD = {res_std}\")\n",
        "    # check three-sigma rule\n",
        "    one_sigma = count_items_in_interval(all_residual,res_avg-res_std,res_avg+res_std)\n",
        "    two_sigma = count_items_in_interval(all_residual,res_avg-2*res_std,res_avg+2*res_std)\n",
        "    three_sigma = count_items_in_interval(all_residual,res_avg-3*res_std,res_avg+3*res_std)\n",
        "    print(f\"one sigma percent: {one_sigma}, two sigma percent: {two_sigma}, three sigma percent: {three_sigma}\")\n",
        "    # np.histogram  range=-200~200, bins=? ?\n",
        "    hist,bins = np.histogram(a=all_residual,bins=10,range=(-200,200),density=False)\n",
        "    plt.clf()\n",
        "    # \n",
        "    mu = res_avg\n",
        "    sigma = res_std\n",
        "    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 10000)\n",
        "    plt.plot(x, stats.norm.pdf(x, mu, sigma), color=\"#FF0000\")\n",
        "    #  range, binsn&m, density=True \n",
        "    plt.hist(all_residual, bins=80, range=(-100,100), facecolor=\"blue\", edgecolor=\"black\", alpha=0.7, density=True)\n",
        "    plt.xlabel(\"Residual\")\n",
        "    plt.ylabel(\"Probability\")\n",
        "    plt.savefig(f'/content/drive/MyDrive/Colab Notebooks/pdf_node_{nodes}.png')\n",
        "    plt.show()\n",
        "    # z score\n",
        "    mu = 0\n",
        "    sigma = 1\n",
        "    z_residual = stats.zscore(all_residual)\n",
        "    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 10000)\n",
        "    plt.plot(x, stats.norm.cdf(x, mu, sigma), color=\"#FF0000\")\n",
        "    plt.hist(z_residual, bins=50, range=(-3,3), facecolor=\"#007500\", edgecolor=\"black\", alpha=0.7, density=True, cumulative=True)\n",
        "    plt.xlabel(\"Residual Z score\")\n",
        "    plt.ylabel(\"Cumulative Probability\")\n",
        "    plt.savefig(f'/content/drive/MyDrive/Colab Notebooks/cdf_node_{nodes}.png')\n",
        "    plt.show()\n",
        "    return res_avg, res_std, X_streaming, y_streaming, model"
      ],
      "metadata": {
        "id": "ImNy3o2afHI7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EDD:\n",
        "    OUTLIERS_THRESHOLD = 2.0\n",
        "    # based on previous testing, errors mean = 0, STD = 20\n",
        "    def __init__(self, mean: np.float64 = np.float64(0.), std: np.float64 = np.float64(20.), window_size: int=100, percent: float=0.2, decay_rate: float = 0.2):\n",
        "        self.percent = percent\n",
        "        self.window_size = window_size\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.window = [0]*window_size\n",
        "        self.X_data = np.array([np.zeros(6)])\n",
        "        self.y_data = np.array([np.zeros(1)])\n",
        "        # time weighting\n",
        "        self.decay_rate = decay_rate\n",
        "        # Records current sequential outliers\n",
        "        self.set_threshold()\n",
        "        self.set_weighted_score_window()\n",
        "        self.set_window_score_threshold()\n",
        "\n",
        "    # construct an exponential decay ndarray with window_size w\n",
        "    def set_weighted_score_window(self):\n",
        "        # Calculate the time indices such that t = 0 corresponds to the last element\n",
        "        t = np.arange(self.window_size-1, -1, -1)\n",
        "        # Apply the exponential decay\n",
        "        decay_factors = np.exp(-self.decay_rate * t)\n",
        "        weighted_score_window = np.ones(self.window_size) * decay_factors\n",
        "        self.weighted_score_window = weighted_score_window\n",
        "\n",
        "    # set the threshold of the window score\n",
        "    def set_window_score_threshold(self):\n",
        "        full_outliers = [1]*self.window_size\n",
        "        max_window_score = sum(self.weighted_score_window*full_outliers)\n",
        "        self.window_score_threshold = max_window_score*self.percent\n",
        "\n",
        "    def set_threshold(self):\n",
        "        # outlier threshold\n",
        "        self.upper = self.mean+self.OUTLIERS_THRESHOLD*self.std\n",
        "        self.lower = self.mean-self.OUTLIERS_THRESHOLD*self.std\n",
        "\n",
        "    def reset_distribution(self, mean: float=0, std: float=20):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def reset_window(self):\n",
        "        self.X_data = np.array([np.zeros(6)])\n",
        "        self.y_data = np.array([np.zeros(1)])\n",
        "        self.window = [0]*window_size\n",
        "\n",
        "    def add_data(self, error: np.float64, X: np.float64, y: np.float64) -> Tuple[bool, np.ndarray, np.ndarray]:\n",
        "        # determine is an outlier or not\n",
        "        result = 0 if self.upper>error and self.lower<error else 1\n",
        "        # save X, y data for retrain\n",
        "        if not self.X_data.any() and not self.y_data.any():\n",
        "            self.X_data+=X\n",
        "            self.y_data+=y\n",
        "        elif len(self.X_data)<self.window_size:\n",
        "            self.X_data = np.append(self.X_data, X, axis=0)\n",
        "            self.y_data = np.append(self.y_data, np.array([[y]]), axis=0)\n",
        "        elif len(self.X_data)>self.window_size or len(self.y_data)>self.window_size:\n",
        "            raise Exception(\"Invalid Data Size\")\n",
        "        else:\n",
        "            self.X_data = np.append(self.X_data, X, axis=0)\n",
        "            self.X_data = self.X_data[1:]\n",
        "            self.y_data = np.append(self.y_data, np.array([[y]]), axis=0)\n",
        "            self.y_data = self.y_data[1:]\n",
        "        if len(self.window)!=self.window_size:\n",
        "            raise Exception(\"Invalid Window Size\")\n",
        "        else:\n",
        "            self.window.append(result)\n",
        "            self.window.pop(0)\n",
        "            detected = self.detect()\n",
        "        return (detected, self.X_data, self.y_data)\n",
        "\n",
        "    def detect(self) -> bool:\n",
        "        # total score of outliers\n",
        "        window_score = sum(self.weighted_score_window*self.window)\n",
        "        if window_score >= self.window_score_threshold:\n",
        "            print(f\"upper: {self.upper}, lower: {self.lower}, score: {window_score}, Structural Change has been detected\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # calculate window_score of previous model\n",
        "    def get_window_score(self, errors: np.ndarray)->Tuple[bool, float]:\n",
        "        temp_window = [0]*self.window_size\n",
        "        for error in errors:\n",
        "            error = error[0]\n",
        "            result = 0 if self.upper>error and self.lower<error else 1\n",
        "            temp_window.append(result)\n",
        "            temp_window.pop(0)\n",
        "        window_score = sum(self.weighted_score_window*temp_window)\n",
        "        if window_score >= self.window_score_threshold:\n",
        "            return (True, window_score)\n",
        "        return (False, window_score)"
      ],
      "metadata": {
        "id": "fMojb8DL1dQF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from river import drift\n",
        "\n",
        "class DDM:\n",
        "    OUTLIERS_THRESHOLD = 2.0\n",
        "    def __init__(self, mean: np.float64 = np.float64(0.), std: np.float64 = np.float64(20.), warm_start:int=30, warning_threshold:float=2.0, drift_threshold:float=3.0):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.DDM = drift.binary.DDM(warm_start=warm_start, warning_threshold=warning_threshold, drift_threshold=drift_threshold)\n",
        "        self.set_threshold()\n",
        "\n",
        "    def set_threshold(self):\n",
        "        # outlier threshold\n",
        "        self.upper = self.mean+self.OUTLIERS_THRESHOLD*self.std\n",
        "        self.lower = self.mean-self.OUTLIERS_THRESHOLD*self.std\n",
        "\n",
        "    def add_data(self, error: np.float64)->Tuple[bool, bool]:\n",
        "        result = False if self.upper>error and self.lower<error else True\n",
        "        warning_level = False\n",
        "        self.DDM.update(result)\n",
        "        return self.DDM.warning_detected, self.DDM.drift_detected"
      ],
      "metadata": {
        "id": "fqwAqgtA_pqh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from river import drift\n",
        "\n",
        "class EDDM:\n",
        "    OUTLIERS_THRESHOLD = 2.0\n",
        "    def __init__(self, mean: np.float64 = np.float64(0.), std: np.float64 = np.float64(20.), warm_start:int=30, alpha:float=0.95, beta:float=0.9):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.EDDM = drift.binary.EDDM(warm_start=warm_start, alpha=alpha, beta=beta)\n",
        "        self.set_threshold()\n",
        "\n",
        "    def set_threshold(self):\n",
        "        # outlier threshold\n",
        "        self.upper = self.mean+self.OUTLIERS_THRESHOLD*self.std\n",
        "        self.lower = self.mean-self.OUTLIERS_THRESHOLD*self.std\n",
        "\n",
        "    def add_data(self, error: np.float64)->Tuple[bool, bool]:\n",
        "        result = False if self.upper>error and self.lower<error else True\n",
        "        warning_level = False\n",
        "        self.EDDM.update(result)\n",
        "        return self.EDDM.warning_detected, self.EDDM.drift_detected"
      ],
      "metadata": {
        "id": "AbxNii1Horhb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# remove all pickle file\n",
        "def delete_files_in_directory_and_subdirectories(directory_path):\n",
        "    try:\n",
        "        for root, dirs, files in os.walk(directory_path):\n",
        "            print(files)\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                os.remove(file_path)\n",
        "        print(\"All files and subdirectories deleted successfully.\")\n",
        "    except OSError:\n",
        "        print(\"Error occurred while deleting files and subdirectories.\")"
      ],
      "metadata": {
        "id": "xNOQ_GZxMpfV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_fig(real_data:Iterable, predictions:Iterable, drift_points: list, drift_detect_index: list, clean: bool = True, colors: tuple=('#2828FF', '#FF0000', '#000000', '#FF8000')):\n",
        "    plt.figure(figsize=(14,7))\n",
        "    for index, detect_index in enumerate(drift_detect_index):\n",
        "        plt.axvline(x=detect_index, color=colors[0], linestyle=':', label=f'Drift Detected at {index+1}')\n",
        "    for index, drift_point in enumerate(drift_points):\n",
        "        plt.axvline(x=drift_point, color=colors[1], linestyle='--', label=f'Concept Drift Point {index+1}')\n",
        "    label1 = \"Real data\"\n",
        "    label2 = \"Prediction data\"\n",
        "    color1 = colors[2]\n",
        "    color2 = colors[3]\n",
        "    plt.plot(real_data, label=label1, color=color1, linestyle='-', linewidth=2)\n",
        "    plt.plot(predictions, label=label2, color=color2, alpha=0.7, linestyle='-',linewidth=2)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    if clean:\n",
        "        plt.clf()"
      ],
      "metadata": {
        "id": "QWSFA9K3CQ8a"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EDD_sudden_drift_test(window_size: int = 100, percent: float = 0.8, nodes: int = 60, decay_rate: float = 0.2):\n",
        "    avg, std, X_streaming, y_streaming, elmr = train_part(nodes)\n",
        "    edd = EDD(mean=avg, std=std, window_size=window_size, percent=percent, decay_rate=decay_rate)\n",
        "    # record all model\n",
        "    model_nums = set()\n",
        "    y_predictions = list()\n",
        "    drift_detect_index = list()\n",
        "\n",
        "    # drift point\n",
        "    closet_drift_index = len(y_streaming)\n",
        "\n",
        "    for index, X in enumerate(X_streaming):\n",
        "        X = X.reshape(-1, 6)\n",
        "        X = multi_sc.transform(X)\n",
        "        y_pred = elmr.predict(X)\n",
        "        y = y_streaming[index]\n",
        "        error = y_pred[0][0] - y\n",
        "        y_predictions.append(y_pred[0][0])\n",
        "        # print(f\"pred {y_pred[0][0]}, actual {y}\")\n",
        "\n",
        "        detected, window_X_data, window_y_data = edd.add_data(error, X, y)\n",
        "        if detected:\n",
        "            print(f\"\\nError at index {index}, daily price: {y}\\n\")\n",
        "            drift_detect_index.append(index)\n",
        "            edd.reset_window()\n",
        "\n",
        "            if index<closet_drift_index:\n",
        "                closet_drift_index = index\n",
        "            if index<drift_points[0] or len(drift_detect_index)>=2:\n",
        "                closet_drift_index = 1000\n",
        "                break\n",
        "            elmr.partial_fit(window_X_data, window_y_data)\n",
        "\n",
        "    plot_fig(y_streaming, y_predictions, drift_points, drift_detect_index)\n",
        "    return closet_drift_index"
      ],
      "metadata": {
        "id": "xe6VSYaP4ySG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DDM_sudden_drift_test(nodes: int = 60):\n",
        "    avg, std, X_streaming, y_streaming, elmr = train_part(nodes)\n",
        "    # init DDM\n",
        "    ddm = DDM(mean=avg, std=std, drift_threshold=3.0)\n",
        "    # record all model\n",
        "    y_predictions = list()\n",
        "    drift_detect_index = list()\n",
        "\n",
        "    # drift point\n",
        "    closet_drift_index = len(y_streaming)\n",
        "\n",
        "    for index, X in enumerate(X_streaming):\n",
        "        X = X.reshape(-1, 6)\n",
        "        X = multi_sc.transform(X)\n",
        "        y_pred = elmr.predict(X)\n",
        "        y = y_streaming[index]\n",
        "        error = y_pred[0][0] - y\n",
        "        y_predictions.append(y_pred[0][0])\n",
        "\n",
        "        # DDM detection\n",
        "        warning_detected, drift_detected = ddm.add_data(error)\n",
        "        if drift_detected:\n",
        "            print(f\"\\nDDM detect error at index {index}, daily price: {y}\\n\")\n",
        "            drift_detect_index.append(index)\n",
        "            if index<closet_drift_index:\n",
        "                closet_drift_index = index\n",
        "            if index<drift_points[0] or len(drift_detect_index)>=2:\n",
        "                closet_drift_index = 1000\n",
        "                break\n",
        "\n",
        "    colors = ('#8600FF', '#FF0000', '#000000', '#00EC00')\n",
        "    plot_fig(y_streaming, y_predictions, drift_points, drift_detect_index, colors=colors)\n",
        "    return closet_drift_index"
      ],
      "metadata": {
        "id": "T1igwIoAFz70"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EDDM_sudden_drift_test(nodes: int = 60):\n",
        "    avg, std, X_streaming, y_streaming, elmr = train_part(nodes)\n",
        "    # init EDDM\n",
        "    eddm = EDDM(mean=avg, std=std, alpha=0.95, beta=0.9)\n",
        "    # record all model\n",
        "    y_predictions = list()\n",
        "    drift_detect_index = list()\n",
        "\n",
        "    # drift point\n",
        "    closet_drift_index = len(y_streaming)\n",
        "\n",
        "    for index, X in enumerate(X_streaming):\n",
        "        X = X.reshape(-1, 6)\n",
        "        X = multi_sc.transform(X)\n",
        "        y_pred = elmr.predict(X)\n",
        "        y = y_streaming[index]\n",
        "        error = y_pred[0][0] - y\n",
        "        y_predictions.append(y_pred[0][0])\n",
        "\n",
        "        # DDM detection\n",
        "        warning_detected, drift_detected = eddm.add_data(error)\n",
        "        if drift_detected:\n",
        "            print(f\"\\nEDDM detect error at index {index}, daily price: {y}\\n\")\n",
        "            drift_detect_index.append(index)\n",
        "            if index<closet_drift_index:\n",
        "                closet_drift_index = index\n",
        "            if index<drift_points[0] or len(drift_detect_index)>=2:\n",
        "                closet_drift_index = 1000\n",
        "                break\n",
        "\n",
        "    colors = ('#EA7500', '#FF0000', '#000000', '#BE77FF')\n",
        "    plot_fig(y_streaming, y_predictions, drift_points, drift_detect_index, colors=colors)\n",
        "    return closet_drift_index"
      ],
      "metadata": {
        "id": "YZ11-qRjpU2R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EDD_recurring_drift_test(window_size: int = 100, percent: float = 0.8, nodes: int = 60, decay_rate: float = 0.2):\n",
        "    avg, std, X_streaming, y_streaming, elmr = train_part(nodes)\n",
        "    edd = EDD(mean=avg, std=std, window_size=window_size, percent=percent, decay_rate=decay_rate)\n",
        "    # record all model\n",
        "    model_nums = set()\n",
        "    y_predictions = list()\n",
        "    drift_detect_index = list()\n",
        "\n",
        "    for index, X in enumerate(X_streaming):\n",
        "        X = X.reshape(-1, 6)\n",
        "        X = multi_sc.transform(X)\n",
        "        y_pred = elmr.predict(X)\n",
        "        y = y_streaming[index]\n",
        "        error = y_pred[0][0] - y\n",
        "        y_predictions.append(y_pred[0][0])\n",
        "\n",
        "        detected, window_X_data, window_y_data = edd.add_data(error, X, y)\n",
        "        if detected:\n",
        "            print(f\"\\nError at index {index}, daily price: {y}\\n\")\n",
        "            drift_detect_index.append(index)\n",
        "            filehandler = open(f\"/content/drive/MyDrive/Colab Notebooks/model_repository/model_{index}.pkl\",\"wb\")\n",
        "            pickle.dump(elmr,filehandler)\n",
        "            filehandler.close()\n",
        "            edd.reset_window()\n",
        "\n",
        "            if model_nums and len(model_nums)<=10:\n",
        "                best_window_score = window_size\n",
        "                failed_count = 0\n",
        "                for model_num in model_nums:\n",
        "                    model_file = open(f\"/content/drive/MyDrive/Colab Notebooks/model_repository/model_{model_num}.pkl\",'rb')\n",
        "                    prev_oselmr = pickle.load(model_file)\n",
        "                    prev_y_pred = prev_oselmr.predict(window_X_data)\n",
        "                    prev_errors = prev_y_pred-window_y_data\n",
        "                    prev_detected, prev_window_score = edd.get_window_score(prev_errors)\n",
        "                    model_file.close()\n",
        "                    if not prev_detected and prev_window_score<=best_window_score:\n",
        "                        # reuse previous model\n",
        "                        best_window_score = prev_window_score\n",
        "                        print(f\"Recurring Drift Detect, previous model with score {prev_window_score} and mean errors {np.mean(prev_errors)} and name model_{model_num}.pkl\")\n",
        "                        elmr = prev_oselmr\n",
        "                    else:\n",
        "                        print(f\"Previous model with score {prev_window_score} and mean errors {np.mean(prev_errors)} and name model_{model_num}.pkl\")\n",
        "                        failed_count+=1\n",
        "                # if no model fit current concept, update current model\n",
        "                if failed_count==len(model_nums):\n",
        "                    elmr.partial_fit(window_X_data, window_y_data)\n",
        "            else:\n",
        "                # if there is no model in repostiory, update current model\n",
        "                elmr.partial_fit(window_X_data, window_y_data)\n",
        "            model_nums.add(index)\n",
        "    # remove all pickle file\n",
        "    directory_path = '/content/drive/MyDrive/Colab Notebooks/model_repository'\n",
        "    delete_files_in_directory_and_subdirectories(directory_path)\n",
        "    plot_fig(y_streaming, y_predictions, drift_points, drift_detect_index)"
      ],
      "metadata": {
        "id": "pDeFYmNNJ7C5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZy7bywzGFkn"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "if __name__ == \"__main__\":\n",
        "    # EDD parameters\n",
        "    window_size = 100\n",
        "    percent = 0.8\n",
        "    nodes = 60\n",
        "    decay_rate = 0.1\n",
        "    drift_points = [400]\n",
        "    min_dp = 1000\n",
        "    min_index = 0\n",
        "    for i in range(10):\n",
        "        print(f\"-------------------------------------------Experiment {i}----------------------------------------------\")\n",
        "        # EDD_recurring_drift_test(window_size, percent, nodes, decay_rate)\n",
        "        dp = EDD_sudden_drift_test(window_size, percent, nodes, decay_rate)\n",
        "        dp2 = DDM_sudden_drift_test(nodes)\n",
        "        dp3 = EDDM_sudden_drift_test(nodes)\n",
        "        if dp<min_dp:\n",
        "            min_dp = dp\n",
        "            min_index = i\n",
        "    print(f\"min drift point = {min_dp} and index = {min_index}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}